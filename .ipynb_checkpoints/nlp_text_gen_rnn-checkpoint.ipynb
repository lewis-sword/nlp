{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27999dd4-9500-451e-83ae-8f30e6cf9cb9",
   "metadata": {},
   "source": [
    "# Recurrent neural network for text generation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e3c384-da96-460a-b0d7-ce2e84e47a92",
   "metadata": {},
   "source": [
    "- The Keras embedding layer uses a word embedding based on integer encoded inputs, i.e. each word in the vocabulary is represented by an integer, and then this is processed. So it at least initially differs to that of word2vec, since word2vec uses one hot encoding where each word is represented by a vocabulary sized vector of all zeros expect a single 1.\n",
    "- Intro: https://machinelearningmastery.com/an-introduction-to-recurrent-neural-networks-and-the-math-that-powers-them/\n",
    "- Based on setup from https://www.tensorflow.org/text/tutorials/text_generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf29b2e6-8d4d-4c4e-86dd-52252be9a982",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import io\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f60b4a59-b3a2-4a42-a11c-4db99989aa92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1115394 characters\n"
     ]
    }
   ],
   "source": [
    "url='https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt'\n",
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', url)\n",
    "# Read, then decode for py2 compat.\n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "# length of text is the number of characters in it\n",
    "print(f'Length of text: {len(text)} characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0159a3a5-62f6-4fd3-b9ab-d0c9f8ded4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternatively, read it in through pandas\n",
    "s=requests.get(url).content\n",
    "c=pd.read_table(io.StringIO(s.decode('utf-8')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d3e2db5-bddf-4db1-8142-016459e6ea1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      First Citizen:\n",
      "0      Before we proceed any further, hear me speak.\n",
      "1                                               All:\n",
      "2                                      Speak, speak.\n",
      "3                                     First Citizen:\n",
      "4  You are all resolved rather to die than to fam...\n"
     ]
    }
   ],
   "source": [
    "print(c.head())\n",
    "#print(c.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8aed9ff4-4bc0-4ee5-b926-d44613c271ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "\n",
      "65 unique characters\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the first 250 characters in text\n",
    "print(text[:80])\n",
    "# The unique characters in the file\n",
    "vocab = sorted(set(text))\n",
    "print(\"\\n\")\n",
    "print(f'{len(vocab)} unique characters')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdf51d1-3722-4fcf-a1ca-69298448066b",
   "metadata": {},
   "source": [
    "Embedding the strings so that each character has a numerical value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ad7b53a-aac4-4d17-a79a-5d1ad97da033",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b' ', b'a', b'b', b'c', b'q', b'r', b's', b'e', b'd', b' ', b'e', b'f',\n",
       "  b'g', b',']                                                            ,\n",
       " [b'x', b'y', b'z']]>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_texts = [' abcqrsed efg,', 'xyz']\n",
    "\n",
    "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "247e0354-09a6-4b1e-ae64-f142961fc7c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[2, 40, 41, 42, 56, 57, 58, 44, 43, 2, 44, 45, 46, 7], [63, 64, 65]]>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids_from_chars = tf.keras.layers.StringLookup(\n",
    "    vocabulary=list(vocab), mask_token=None)\n",
    "\n",
    "# ids will print out the string in terms of its integer. Remember there are only 65 unique characters in the Shakespeare piece, including \n",
    "# punctuation and other symbols like spaces\n",
    "ids = ids_from_chars(chars)\n",
    "ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006ce817-a39c-47a9-bf85-762ec0cdbadf",
   "metadata": {},
   "source": [
    "Note our network is going to reutrn numerical values, so naturally we want to convert these back to their corpus values (the 65 unqiue characters) and we can do so using tf.keras.layers.StringLookup(..., invert=True). E.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45d5e7ca-a139-4464-b258-7608c3a674ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b' abcqrsed efg,' b'xyz']\n"
     ]
    }
   ],
   "source": [
    "chars_from_ids = tf.keras.layers.StringLookup(\n",
    "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)\n",
    "chars = chars_from_ids(ids)\n",
    "chars\n",
    "print(tf.strings.reduce_join(chars, axis=-1).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5cd270-a458-4a78-aa1a-d71bbe4df6c6",
   "metadata": {},
   "source": [
    "Provided with an input string, the model is trained to predict the next character of the string. We have to create some training data. This is a supervised task since we have an input string which is our x input data, and we also have the character (or string) directly after it which is our y, output data.\n",
    "\n",
    "Each input \"sequence\" we break the text into will contain `seq_length` lots of characters. The target or output data, will contain `seq_length` lots of characters also but it will take the last 4 characters of the sequence. E.g. break text into `seq_length+1 = 5`, example \"Hello\", the input would be \"Hell\", the output/target would be \"ello\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5795d0e1-831c-4983-a55c-28c012b2a445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([19 48 57 ... 46  9  1], shape=(1115394,), dtype=int64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['F', 'i', 'r', 's', 't', ' ', 'C', 'i', 't', 'i']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converting full text into its numerical format.\n",
    "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
    "print(all_ids)\n",
    "\n",
    "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
    "[chars_from_ids(ids).numpy().decode('utf-8') for ids in ids_dataset.take(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2e10629-48e0-4d0b-823f-68ee097009c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_from_ids(ids):\n",
    "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab22d6a1-2f04-4470-8159-daf3202dc309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
      " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
      " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
      " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
      " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
      " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
      " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
      " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "seq_length = 100\n",
    "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for seq in sequences.take(1):\n",
    "  print(chars_from_ids(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "535f930b-3c3c-46e2-a4eb-7ad0e8b047b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
       " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_input_target(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "\n",
    "split_input_target(seq)\n",
    "split_input_target(list(\"Tensorflow\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d09cc6c4-45df-400c-a93c-05230fedffd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_TakeDataset element_spec=(TensorSpec(shape=(100,), dtype=tf.int64, name=None), TensorSpec(shape=(100,), dtype=tf.int64, name=None))>\n"
     ]
    }
   ],
   "source": [
    "dataset = sequences.map(split_input_target)\n",
    "print(dataset.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0efef285-6a2e-4476-aa68-07462f1e4936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
      "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
      "Input : b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you '\n",
      "Target: b're all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
      "Input : b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us k\"\n",
      "Target: b\"ow Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in dataset.take(3):\n",
    "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
    "    print(\"Target:\", text_from_ids(target_example).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "119fef3b-1a7b-4032-ba49-e3ecf57d1ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 3000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ed4a2b4-0c47-4fcf-965f-44dd0cf85ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_PrefetchDataset element_spec=(TensorSpec(shape=(32, 100), dtype=tf.int64, name=None), TensorSpec(shape=(32, 100), dtype=tf.int64, name=None))>\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d19f462-cf0c-44cb-9c2d-8671c08daff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32,)\n",
      "Input : b\"g'st sweet music. Hark, come hither, Tyrrel\\nGo, by this token: rise, and lend thine ear:\\nThere is no\"\n",
      "Target: b\"'st sweet music. Hark, come hither, Tyrrel\\nGo, by this token: rise, and lend thine ear:\\nThere is no \"\n"
     ]
    }
   ],
   "source": [
    "#Randomly selects parts of the text\n",
    "for input_example, target_example in dataset.take(1):\n",
    "    print(text_from_ids(input_example).numpy().shape)\n",
    "    print(\"Input :\", text_from_ids(input_example).numpy()[0]) # Note we only take one from the batch\n",
    "    print(\"Target:\", text_from_ids(target_example).numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "739bb66b-93a1-4493-9795-b3911ac2b841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in StringLookup Layer\n",
    "vocab_size = len(ids_from_chars.get_vocabulary())\n",
    "# The embedding dimension. Embedding layer creates a vector representation of each member of the vocabulary. The vector's size is size embedding_dim\n",
    "embedding_dim = 128\n",
    "# Number of RNN units\n",
    "rnn_units = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cce075-fefa-4997-b18d-c7d389b4923a",
   "metadata": {},
   "source": [
    "GRU is Gated Recurrent Unit. Each GRU takes two inputs: previous GRU state value and character embedding input. Each GRU output feeds the consequent GRU and also passes output to the dense layer, after which it will be converted into the logits for each vocab member."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0e03fe-d8b4-4c68-8a9c-0483e31384ce",
   "metadata": {},
   "source": [
    "Having set up the input data, we employ the RNN model.\n",
    " - super() allows us to initialize the attributes of the parent class.\n",
    " - we also define the three core layers: embedding, gru and dense for output. Note the dense layer has `vocab_size` number of nodes, ready to return a `vocab_size` set of logits, so that when we convert to probabilities, we can pick out which term in the vocabulary is the most likely in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "821154c2-85c7-4eb0-a4d9-ac12a4baf806",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "    super().__init__(self)\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True) # GRU has output vector dim = rnn_units\n",
    "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "  def call(self, inputs, states=None, return_state=False, training=False):\n",
    "    x = inputs\n",
    "    x = self.embedding(x, training=training)\n",
    "    if states is None:\n",
    "      states = self.gru.get_initial_state(x)\n",
    "    x, states = self.gru(x, initial_state=states, training=training)\n",
    "    x = self.dense(x, training=training)\n",
    "\n",
    "    if return_state:\n",
    "      return x, states\n",
    "    else:\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a123898b-2e2c-4f11-b990-f8c7567d4d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we inistantiate model\n",
    "model = MyModel(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2aeffa77-90f7-4ae2-a915-e67f35a10d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch) #passing the input data through the model.\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d69d7978-9330-4343-9156-9c99a25707ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 66)\n",
      "[[-0.00244161 -0.00189861  0.01936328 ... -0.01602551  0.00797732\n",
      "  -0.00967796]\n",
      " [ 0.00205034 -0.00417172  0.00668585 ... -0.00462392 -0.00467548\n",
      "  -0.01352196]\n",
      " [ 0.01252944  0.00064899 -0.00723644 ... -0.01087652  0.00205605\n",
      "  -0.00039377]\n",
      " ...\n",
      " [ 0.00885282 -0.01293257  0.00142927 ... -0.01265633  0.02160918\n",
      "   0.00436883]\n",
      " [-0.00017103 -0.0147817   0.00864164 ... -0.01568041  0.00127978\n",
      "  -0.00010806]\n",
      " [ 0.00431343 -0.01233269  0.00233574 ... -0.00498666 -0.00767525\n",
      "  -0.00856074]]\n",
      "[54 15 44 44 13 13 31 49 64 45 91 23 67 35 87 92 70 86 49 85 37 46 77  6\n",
      " 43 70 34 44 71 72 11 85 23 12 49  9 16 73 34 47 91 47  8 10 39 54 71 28\n",
      " 41 91 70 29 70 48 75 97 72 77 66 98 67 28 92 70 97 74]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b\"oBee??Rjyf[UNK]J[UNK]V[UNK][UNK][UNK][UNK]j[UNK]Xg[UNK]'d[UNK]Ue[UNK][UNK]:[UNK]J;j.C[UNK]Uh[UNK]h-3Zo[UNK]Ob[UNK][UNK]P[UNK]i[UNK][UNK][UNK][UNK][UNK][UNK][UNK]O[UNK][UNK][UNK][UNK]\">"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_out =example_batch_predictions.numpy()[1]\n",
    "print(example_out.shape) # Each character in the sequence, has a 66 length vector attached to it, these are the logits predictions for what the next \n",
    "    # character should be in the sequence.\n",
    "#print(np.argmax(example_out))\n",
    "print(example_out)\n",
    "logits_preds=np.argmax(example_out, axis=0)\n",
    "print(logits_preds)\n",
    "chars_from_ids(logits_preds) # Looks nonsensical as output, but after all the model hasn't been trained, so the weights aren't tuned\n",
    "text_from_ids(logits_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6f52b15e-9803-421a-8ef1-c117d30fa36a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      " b\"lt die, by God's just ordinance,\\nEre from this war thou turn a conqueror,\\nOr I with grief and extrem\"\n",
      "\n",
      "Next Char Predictions:\n",
      " b\"pc3.hHoLcesCoSjzfZ[UNK].xzY'wceUndHpmR?Tjj,Nh?b!DaK&Efwc&hjv[UNK]Blfpkamsm:DCTrHEVHtyBbIg J;[UNK]OEmYiCHOTcCYD.!\"\n"
     ]
    }
   ],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n",
    "\n",
    "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
    "print()\n",
    "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289cf0ff-433f-4d6e-902d-2ea77fdd874d",
   "metadata": {},
   "source": [
    " - Now define the loss function so that we can start training the model. The ouput of the model, as seen in the class MyModel definition, is a dense layer of nodes. Since there is no activation function on that layer, the output is just logits. The loss function to compare the output layer to the true/target value is then sparse categorical crossentropy.\n",
    " - Finally we compile the network using the adam optimiser to tune the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5865c0e5-fbea-4a7b-ab72-fd1c91e97987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (32, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
      "Mean loss:         tf.Tensor(4.1886606, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"Mean loss:        \", example_batch_mean_loss)\n",
    "\n",
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "25b16f3e-a94b-40a1-96b2-8889fdc69d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "345/345 [==============================] - 139s 396ms/step - loss: 2.5406\n",
      "Epoch 2/10\n",
      "345/345 [==============================] - 170s 492ms/step - loss: 1.8805\n",
      "Epoch 3/10\n",
      "345/345 [==============================] - 162s 469ms/step - loss: 1.6417\n",
      "Epoch 4/10\n",
      "345/345 [==============================] - 161s 465ms/step - loss: 1.5138\n",
      "Epoch 5/10\n",
      "345/345 [==============================] - 162s 467ms/step - loss: 1.4366\n",
      "Epoch 6/10\n",
      "345/345 [==============================] - 170s 491ms/step - loss: 1.3845\n",
      "Epoch 7/10\n",
      "345/345 [==============================] - 155s 449ms/step - loss: 1.3446\n",
      "Epoch 8/10\n",
      "345/345 [==============================] - 170s 492ms/step - loss: 1.3114\n",
      "Epoch 9/10\n",
      "345/345 [==============================] - 158s 457ms/step - loss: 1.2823\n",
      "Epoch 10/10\n",
      "345/345 [==============================] - 159s 460ms/step - loss: 1.2564\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "history = model.fit(dataset, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2758241-a8a4-4502-b4d0-1b0e1eef5e9c",
   "metadata": {},
   "source": [
    "Miniature code after running epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "32a32e00-edd5-4571-9aa4-533cfb6ebb24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits for each given vocabulary token:  tf.Tensor(\n",
      "[[-10.76817      4.124313     6.5988517   -0.55192953  -7.743239\n",
      "   -7.96808      1.1384192    0.7626041   -1.5175424   -1.8090538\n",
      "   -7.461964    -1.9752158   -2.0393474   -0.16406521  -6.644651\n",
      "   -2.7755587   -1.2671316  -12.325125   -16.959589    -5.678003\n",
      "   -4.1277566   -6.0240493  -10.284635    -5.0300655   -5.7403293\n",
      "   -4.445474    -1.9379029   -5.581869    -5.4057302   -2.1415179\n",
      "   -6.220854    -1.4235003   -2.9025276   -1.5184338   -9.805831\n",
      "   -8.0391      -4.2209473   -8.74412     -7.2839494   -7.2051206\n",
      "   -2.3716993   -0.9391699   -1.0404209   -0.88366956   4.481169\n",
      "   -3.888344    -4.516986    -8.12911      2.6696954   -6.8614583\n",
      "   -1.3988788    0.56288576   5.4198403    3.2455704    5.134699\n",
      "    1.849685    -5.566955     5.5726066   10.199316     2.081392\n",
      "   13.586778    -3.8270469    2.454413    -5.724052    -0.34534848\n",
      "   -4.9702806 ]], shape=(1, 66), dtype=float32)\n",
      "[60, 60]\n",
      "Predicted characters:  [b'u']\n"
     ]
    }
   ],
   "source": [
    "input_chars = tf.strings.unicode_split([\"ROMEO: Hath tho\"], 'UTF-8')\n",
    "input_ids=ids_from_chars(input_chars).to_tensor()\n",
    "predicted_logits, states = model(inputs=input_ids,return_state=True)\n",
    "predicted_logits = predicted_logits[:, -1, :]\n",
    "print(\"Logits for each given vocabulary token: \", predicted_logits)\n",
    "#print(predicted_logits.numpy()[0][60])\n",
    "predicted_ids = tf.random.categorical(predicted_logits, num_samples=1) # equivalent to calling argmax in some sense, it is more like randomly sampling\n",
    "predicted_ids_2 = np.argmax(predicted_logits) \n",
    "print([predicted_ids.numpy()[0][0],predicted_ids_2]) # Note: ouput logits can be positive or negative, and we take the largest positive.\n",
    "#print(predicted_ids.numpy()[0])\n",
    "predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "predicted_chars = chars_from_ids(predicted_ids)\n",
    "print(\"Predicted characters: \", predicted_chars.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150456b1-a0e6-4404-9b8b-c81503c15a5f",
   "metadata": {},
   "source": [
    "The tensor flow example also provides an iterable process to generate a set of text as seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "940c8563-e044-47f6-9a3f-ce26ac4adf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneStep(tf.keras.Model):\n",
    "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
    "    super().__init__()\n",
    "    self.temperature = temperature\n",
    "    self.model = model\n",
    "    self.chars_from_ids = chars_from_ids\n",
    "    self.ids_from_chars = ids_from_chars\n",
    "\n",
    "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
    "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
    "    sparse_mask = tf.SparseTensor(\n",
    "        # Put a -inf at each bad index.\n",
    "        values=[-float('inf')]*len(skip_ids),\n",
    "        indices=skip_ids,\n",
    "        # Match the shape to the vocabulary\n",
    "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
    "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "\n",
    "  @tf.function\n",
    "  def generate_one_step(self, inputs, states=None):\n",
    "    # Convert strings to token IDs.\n",
    "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "\n",
    "    # Run the model.\n",
    "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
    "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
    "                                          return_state=True)\n",
    "    # Only use the last prediction.\n",
    "    predicted_logits = predicted_logits[:, -1, :]\n",
    "    predicted_logits = predicted_logits/self.temperature\n",
    "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
    "    predicted_logits = predicted_logits + self.prediction_mask\n",
    "    \n",
    "    # Sample the output logits to generate token IDs.\n",
    "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "      \n",
    "    # Convert from token ids to characters\n",
    "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "\n",
    "    # Return the characters and model state.\n",
    "    return predicted_chars, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "09f66165-e7c0-4a6d-9066-349632ada457",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "57460e6d-f37c-4eeb-98ac-145d2a7726a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JULIET: would thou better proud i' the stern?\n",
      "Is all my tribunes. Or with this?\n",
      "\n",
      "ARIEL:\n",
      "It is my marriage, that he they be?\n",
      "Then let the honour perchange your state wheels are patience!\n",
      "And, Trubt, Katharina, with her  \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 0.2006397247314453\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['JULIET: would thou'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(200):\n",
    "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
    "print('\\nRun time:', end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bba320e-95cd-4788-9547-676162cbffda",
   "metadata": {},
   "source": [
    "- Interestingly, even after 2 epochs, it works well enough to capture capitalisation for role name, as well as sentence structure. The meaning and spelling is flawed as would be expected. A greater number of epochs would lead to a higher class of generated outputs.\n",
    "- After 10 epochs the grammar starts to improve too."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
